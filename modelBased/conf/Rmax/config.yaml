R_max:
  R_max: 1
  exploration_timesteps: 1e4
  num_iterations: 1
  exploration_threshold: 10


data_collect:
  env_name: MiniGrid-Empty-8x8-v0 #MiniGrid-Dynamic-Obstacles-8x8-v0 # Name of environment, or comma-separate list of environment names to instantiate as each env in the VecEnv.
  visualize: False # to choose whether your want to see the env during interaction or not
  episodes: 200 # number of episodes to start 
  collect:
    data_train: ${oc.env:TRAIN_DATASET_PATH}/gridworld_Rmax.npz # folder where to save the rollouts
    visit_count: ${oc.env:TRAIN_DATASET_PATH}/visit_count_Rmax.npz # folder where to save the rollouts

world_model:
  model: 'Attention'   # 'Rmax' or 'Attention'
  map_width: 12
  map_height: 6 # for this version, extract the square position centered at the agent 
  action_size: 1 # dimension of the action space
  batch_size: 128
  n_cpu: 8
  hidden_size: 256 #hidden size
  data_dir: ${oc.env:TRAIN_DATASET_PATH}/gridworld_full_augmentation_2.npz
  n_epochs: 50 # number of training epochs
  obs_size: 216 # the env for Rmax height * width * channels = 12 * 6 * 3 and the dimentions of the state is (12=width,6=height,3)
  lr: 1e-4
  wd: 1e-5
  pth_folder: ${oc.env:PTH_FOLDER}/wm_rmax-epoch=40-avg_val_loss_wm=0.0000.ckpt
  # pth_folder: ${oc.env:PTH_FOLDER}/Rmax/world_model_Rmax.ckpt
  obs_norm_values: [10, 5, 3]
  action_norm_values: 6
  valid_values_obj: [1, 2, 4, 5, 8, 10]
  valid_values_color: [0, 1, 5]
  valid_values_state: [0, 1, 2, 3]
  use_wandb: False
  ## params for attention model
  visualizationFlag: True    # whether to visualize the attention map together with the observation
  visualize_every: 1  # 1500 visualize the attention map every n steps
  save_path: ${oc.env:PTH_FOLDER}/AttentionVisulization # save path of attention heatmap
  direction_map: {0: 'right', 1: 'down', 2: 'left', 3: 'up'}
  action_map: {0: 'left', 1: 'right', 2: 'forward', 3: 'pickup', 4: 'drop', 5: 'toggle', 6: 'done'}
  
  attention_model:
    obs_size: 216 # 3 dimensions of state + position of agent
    action_size: 1
    batch_size: 256  # test 64/128/256
    n_cpu: 8
    embed_dim : 128  # 128 64/128/256
    num_heads : 2   # 1 if the embed_dim is 64, num_heads should better be 
    n_epochs: 50 # number of training epochs
    lr: 1e-3
    wd: 1e-5
    model: None
    # pth_folder: ${oc.env:PTH_FOLDER}/world_model.ckpt
    pth_folder: ${oc.env:PTH_FOLDER}/Transformer/extraction_module.ckpt
    obs_norm_values: [10, 5, 3] # object, color, state --> agent_state: 1-down 2-left 3-up 0-right 
    action_norm_values: 6 # 0-left 1-right 2-forward 3-pickup 4-drop 5-toggle 6-done
    valid_values_obj: [1, 2, 4, 5, 8, 10]
    valid_values_color: [0, 1, 5]
    valid_values_state: [0, 1, 2, 3]
    use_wandb: True
    # plot
    visualization_seperate: False  # whether to visualize the attention map
    visualization_together: True    # whether to visualize the attention map together with the observation
    visualize_every: 3000  # 1500 visualize the attention map every n steps
    save_path: ${oc.env:PTH_FOLDER}/AttentionVisulization # save path of attention heatmap
    direction_map: {0: 'right', 1: 'down', 2: 'left', 3: 'up'}
    action_map: {0: 'left', 1: 'right', 2: 'forward', 3: 'pickup', 4: 'drop', 5: 'toggle', 6: 'done'}
    topk: 6



PPO:
  has_continuous_action_space: False
  max_ep_len: 1000  # max timesteps in one episode
  action_std: 0.1  # set same std for action distribution which was used while saving
  i_episode: 0
  K_epochs: 80  # update policy for K epochs
  eps_clip: 0.2  # clip parameter for PPO
  gamma: 0.99  # discount factor
  lr_actor: 0.0003  # learning rate for actor
  lr_critic: 0.001  # learning rate for critic
  checkpoint_path: ${oc.env:PTH_FOLDER}/policy_model_Rmax.ckpt
  time_step: 0
  max_training_timesteps: 5.2e5
  # max_training_timesteps: 3e5
  action_std_decay_freq: 2.5e5
  action_std_decay_rate: 0.05
  min_action_std: 0.1
  save_model_freq: 2e4
  render: True
